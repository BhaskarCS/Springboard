{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Movie Recommendations_PySpark_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO6fnn0sG0nWaZqLaw+70zE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhaskarCS/Springboard/blob/master/Movie_Recommendations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWW9Ze_R1Lqh"
      },
      "source": [
        "# Setup PySpark\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARLuu_P42fUo",
        "outputId": "81530121-5230-4024-ef1e-c300ce6eafa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "# import the SparkConfiguration and SparkContext\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# if we wanted to change any configuration settings for this session only we would define them here\n",
        "conf = SparkConf() \n",
        "\n",
        "# create a SparkContext using the above configuration\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# this command shows the current configuration settings\n",
        "sc._conf.getAll()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.port', '44809'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.driver.host', '2218559182aa'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('spark.app.id', 'local-1603325221672')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir1M1QGJBnOv",
        "outputId": "6722d6a6-f6b6-4903-de82-0095cd085d60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "sc._conf.getAll()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('spark.driver.port', '44809'),\n",
              " ('spark.rdd.compress', 'True'),\n",
              " ('spark.serializer.objectStreamReset', '100'),\n",
              " ('spark.master', 'local[*]'),\n",
              " ('spark.executor.id', 'driver'),\n",
              " ('spark.submit.deployMode', 'client'),\n",
              " ('spark.driver.host', '2218559182aa'),\n",
              " ('spark.ui.showConsoleProgress', 'true'),\n",
              " ('spark.app.name', 'pyspark-shell'),\n",
              " ('spark.app.id', 'local-1603325221672')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awd-Y77O2fX9"
      },
      "source": [
        "# import an SQL spark-session so that we can use dataframes\n",
        "from pyspark.sql import SparkSession\n",
        "# import the ALS algorithm we will be using\n",
        "from pyspark.ml.recommendation import ALS\n",
        "# instantiate the SQL spark session\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqVZ9h6R6Py5",
        "outputId": "bc2ca3e8-0440-458d-9cf5-7c199ec6baee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Point Colaboratory to the Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5ARKiCl2fbm"
      },
      "source": [
        "# read in the data, specifying that there is a header in the csv file and that spark should auto-detect the variable types for each column\n",
        "data = spark.read.option(\"header\",\"true\")\\\n",
        "  .option(\"inferSchema\",\"true\")\\\n",
        "  .format(\"csv\")\\\n",
        "  .load(\"/content/gdrive/My Drive/Colab Notebooks/input/movielens/ratings.csv\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmHdVbEE5PX-",
        "outputId": "e7a5b399-2413-4c08-c2b3-32c41d797b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# print the schema of the dataframe\n",
        "data.printSchema()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- userId: integer (nullable = true)\n",
            " |-- movieId: integer (nullable = true)\n",
            " |-- rating: double (nullable = true)\n",
            " |-- timestamp: integer (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmMSUDKZ2fhh",
        "outputId": "dac14d16-bb59-43e0-ce71-d6bdaeb7acdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# removing timestamp column\n",
        "\n",
        "data = data.drop('timestamp')\n",
        "data.printSchema()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- userId: integer (nullable = true)\n",
            " |-- movieId: integer (nullable = true)\n",
            " |-- rating: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AsqvX3N75ie",
        "outputId": "b18c3525-6798-412b-9be0-c13e9001dbeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We can get spark to show us how many partitions it has split the dataframe up into.\n",
        "data.rdd.getNumPartitions()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc53fpU176FG",
        "outputId": "0a27b4de-e217-4244-a71e-22b031cba8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# shows the top N rows by using the take method:\n",
        "data.take(1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(userId=1, movieId=110, rating=1.0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjupyIxI-PwL"
      },
      "source": [
        "## Model Version 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1cLt91l76oq",
        "outputId": "470109dc-d792-4519-f069-6471909172d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "#instantiate the model, with the \"drop\" cold start strategy\n",
        "model = ALS(coldStartStrategy=\"drop\")\n",
        "\n",
        "# set the column names for the required data\n",
        "model.setItemCol(\"movieId\")\\\n",
        "    .setUserCol(\"userId\")\\\n",
        "    .setRatingCol(\"rating\")\n",
        "\n",
        "# split data into train and test sets with 80:20 proportions\n",
        "(train, test) = data.randomSplit([0.8, 0.2], seed=10)\n",
        "\n",
        "# since the train dataframe will be used many times, forcing spark to cache it could reduce time taken, as we don't have to read from disk as much\n",
        "train.cache()\n",
        "\n",
        "# fit the model to the training set\n",
        "model = model.fit(train)\n",
        "\n",
        "# calculate predictions by using the model to transform the test set\n",
        "predictions = model.transform(test)\n",
        "\n",
        "predictions.printSchema()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- userId: integer (nullable = true)\n",
            " |-- movieId: integer (nullable = true)\n",
            " |-- rating: double (nullable = true)\n",
            " |-- prediction: float (nullable = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAPIsmeY8_Wy"
      },
      "source": [
        "# import the regression evaluator\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# instantiate evaluator, specifying the desired metric \"mae\" and the columns that contain the predictions and the actual values\n",
        "evaluator = RegressionEvaluator(metricName=\"mae\", predictionCol=\"prediction\", labelCol=\"rating\")\n",
        "\n",
        "# evaluate the output of our model\n",
        "mae = evaluator.evaluate(predictions)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHH1t7qY9q_Y"
      },
      "source": [
        "### Model Version 1: Mean Absolute Error: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uDeukEn9AO4",
        "outputId": "bda5fa49-87d8-47b0-8b55-7d0c2ba6fb80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('The Mean Absolute Error is %.3f' % (mae))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Mean Absolute Error is 0.634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT2P33Nk_Rmy"
      },
      "source": [
        "### Model Version 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CiDLMJDGRbh"
      },
      "source": [
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "\n",
        "#create a new ALS estimator\n",
        "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LMM-o7BGVYz"
      },
      "source": [
        "#define a grid for both parameters this will test 9 different combinations of the 2 parameters\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(als.rank, [5, 10, 15]) \\\n",
        "    .addGrid(als.regParam, [1, 0.1, 0.01]) \\\n",
        "    .build()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sln0J_lIGVmu"
      },
      "source": [
        "# split the data with a ratio of 80% training, 20% validation; define the estimator and evaluator to use to determine the best model\n",
        "# also pass in the parameter grid to search over\n",
        "\n",
        "trainValSplit = TrainValidationSplit(estimator = als, estimatorParamMaps=paramGrid, \\\n",
        "                                     evaluator = RegressionEvaluator(metricName=\"mae\", predictionCol=\"prediction\", labelCol=\"rating\"), \\\n",
        "                                     trainRatio = 0.8, parallelism = 4)    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8T7GIPhGZ-i"
      },
      "source": [
        "# fit the model to the training data\n",
        "model = trainValSplit.fit(train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYRrQxqVGZxE"
      },
      "source": [
        "# retrieve the best model\n",
        "bestModel = model.bestModel"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POC2J3ApGnEf"
      },
      "source": [
        "# transform test data using bestModel\n",
        "predictions = bestModel.transform(test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGQ3foaWGnNZ",
        "outputId": "fa94ad2e-f123-4900-bb76-d085b5a90989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# evaluate the predictions\n",
        "mae = evaluator.evaluate(predictions)\n",
        "\n",
        "print('The Mean Absolute Error is %.3f' % (mae)) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Mean Absolute Error is 0.634\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}